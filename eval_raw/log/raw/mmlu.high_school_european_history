nohup: ignoring input
[2025-02-12 21:13:25,635] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:08,  2.71s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:08<00:02,  2.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.65s/it]
Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/lukaemon--mmlu/5407247256b75097c6ed96d65e9673eaf8cb7522ab67e1ea65e7bb85b44be036 (last modified on Wed Feb 12 20:22:24 2025) since it couldn't be found locally at lukaemon/mmlu, or remotely on the Hugging Face Hub.
Generating test split:   0%|          | 0/165 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [00:00<00:00, 6831.79 examples/s]
Generating validation split:   0%|          | 0/18 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 4242.62 examples/s]
Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 672.60 examples/s]
Traceback (most recent call last):
  File "/mnt/deepseekv2/eval_raw/eval_raw_model.py", line 110, in <module>
    result = model.generate(**inputs.to(model.device), max_new_tokens=1)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/transformers/generation/utils.py", line 1622, in generate
    result = self._sample(
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/transformers/generation/utils.py", line 2791, in _sample
    outputs = self(
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-V2-Lite/604d5664dddd88a0433dbae533b7fe9472482de0/modeling_deepseek.py", line 1675, in forward
    outputs = self.model(
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-V2-Lite/604d5664dddd88a0433dbae533b7fe9472482de0/modeling_deepseek.py", line 1544, in forward
    layer_outputs = decoder_layer(
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-V2-Lite/604d5664dddd88a0433dbae533b7fe9472482de0/modeling_deepseek.py", line 1272, in forward
    hidden_states = self.mlp(hidden_states)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-V2-Lite/604d5664dddd88a0433dbae533b7fe9472482de0/modeling_deepseek.py", line 571, in forward
    topk_idx, topk_weight, aux_loss = self.gate(hidden_states)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1595, in _call_impl
    hook_result = hook(self, args, result)
  File "/mnt/deepseekv2/eval_raw/eval_raw_model.py", line 61, in get_layer_output
    expert_idx = output[0].detach().cpu().tolist()
KeyboardInterrupt
